{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Experiments\n",
    "\n",
    "**Tujuan notebook ini:**\n",
    "1. Test berbagai hyperparameter dengan epochs rendah\n",
    "2. Bandingkan learning rates, batch sizes, dan architectures\n",
    "3. Find optimal configuration sebelum full training\n",
    "4. Validate fusion strategies\n",
    "\n",
    "**‚ö†Ô∏è Jalankan notebook ini SETELAH validation notebook dan SEBELUM full training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "\n",
      "üéÆ AMD GPU DETECTED: 2 device(s)\n",
      "‚úÖ GPU memory growth enabled\n",
      "‚úÖ GPU computation test passed\n",
      "üöÄ AMD GPU ready for training!\n",
      "\n",
      "‚úÖ Libraries imported successfully!\n",
      "üéÆ GPU Status: Enabled\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Setup TensorFlow for AMD GPU (DirectML Plugin)\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Setup DirectML Plugin for AMD GPU\n",
    "try:\n",
    "    # Check for GPU and DML devices\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    dml_devices = tf.config.list_physical_devices('DML')\n",
    "    \n",
    "    total_devices = len(gpu_devices) + len(dml_devices)\n",
    "    \n",
    "    if total_devices > 0:\n",
    "        print(f\"\\nüéÆ AMD GPU DETECTED: {total_devices} device(s)\")\n",
    "        \n",
    "        # Configure GPU memory growth\n",
    "        for gpu in gpu_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "        \n",
    "        # Test GPU functionality\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[2.0, 1.0], [1.0, 2.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"‚úÖ GPU computation test passed\")\n",
    "            \n",
    "        gpu_available = True\n",
    "        print(\"üöÄ AMD GPU ready for training!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected. Using CPU mode.\")\n",
    "        gpu_available = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GPU setup failed: {e}\")\n",
    "    print(\"üí° Falling back to CPU mode\")\n",
    "    gpu_available = False\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"\\n‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üéÆ GPU Status: {'Enabled' if gpu_available else 'Disabled (CPU only)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìÅ Load Data\n",
    "\n",
    "Load data yang sudah dipersiapkan untuk training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚úÖ Data loaded successfully!\n",
      "Training samples: 3,123\n",
      "Validation samples: 390\n",
      "Test samples: 391\n",
      "Image shape: (224, 224, 3)\n",
      "Landmark features: 136\n"
     ]
    }
   ],
   "source": [
    "# Base path untuk data\n",
    "BASE_PATH = '../data/'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # Load image data\n",
    "    X_train_images = np.load(BASE_PATH + 'images/X_train_images.npy')\n",
    "    X_val_images = np.load(BASE_PATH + 'images/X_val_images.npy')\n",
    "    X_test_images = np.load(BASE_PATH + 'images/X_test_images.npy')\n",
    "    \n",
    "    # Load landmark data\n",
    "    X_train_landmarks = np.load(BASE_PATH + 'landmarks/X_train_landmarks.npy')\n",
    "    X_val_landmarks = np.load(BASE_PATH + 'landmarks/X_val_landmarks.npy')\n",
    "    X_test_landmarks = np.load(BASE_PATH + 'landmarks/X_test_landmarks.npy')\n",
    "    \n",
    "    # Load labels\n",
    "    y_train = np.load(BASE_PATH + 'images/y_train_images.npy')\n",
    "    y_val = np.load(BASE_PATH + 'images/y_val_images.npy')\n",
    "    y_test = np.load(BASE_PATH + 'images/y_test_images.npy')\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Training samples: {X_train_images.shape[0]:,}\")\n",
    "    print(f\"Validation samples: {X_val_images.shape[0]:,}\")\n",
    "    print(f\"Test samples: {X_test_images.shape[0]:,}\")\n",
    "    print(f\"Image shape: {X_train_images.shape[1:]}\")\n",
    "    print(f\"Landmark features: {X_train_landmarks.shape[1]}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"üí° Please check if data files exist in the correct path\")\n",
    "    print(\"üí° Run data preprocessing first if not done yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6\n",
      "Classes: ['angry', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']\n",
      "Label mapping: {'angry': 0, 'disgusted': 1, 'happy': 2, 'neutral': 3, 'sad': 4, 'surprised': 5}\n",
      "‚úÖ Label conversion completed\n"
     ]
    }
   ],
   "source": [
    "# Create label mapping\n",
    "unique_labels = np.unique(y_train)\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "target_names = list(unique_labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "print(f\"Label mapping: {label_map}\")\n",
    "\n",
    "# Convert labels to numerical\n",
    "y_train_num = np.array([label_map[label] for label in y_train])\n",
    "y_val_num = np.array([label_map[label] for label in y_val])\n",
    "y_test_num = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_num, num_classes)\n",
    "y_val_onehot = to_categorical(y_val_num, num_classes)\n",
    "y_test_onehot = to_categorical(y_test_num, num_classes)\n",
    "\n",
    "print(\"‚úÖ Label conversion completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîß Define Model Architectures\n",
    "\n",
    "Definisikan berbagai variasi model untuk testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model architectures defined!\n"
     ]
    }
   ],
   "source": [
    "def build_cnn_model(input_shape, num_classes, complexity='medium', dropout_rate=0.25):\n",
    "    \"\"\"Build CNN model with different complexity levels\"\"\"\n",
    "    \n",
    "    if complexity == 'simple':\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    elif complexity == 'medium':\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_landmark_model(input_shape, num_classes, complexity='medium', dropout_rate=0.3):\n",
    "    \"\"\"Build landmark model with different complexity levels\"\"\"\n",
    "    \n",
    "    if complexity == 'simple':\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    elif complexity == 'medium':\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üß™ Setup Experiments\n",
    "\n",
    "Setup experiment configurations dan data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ EXPERIMENT SETUP:\n",
      "   Training samples: 2,000\n",
      "   Validation samples: 390\n",
      "   Device: GPU\n",
      "   Configs to test: 8\n"
     ]
    }
   ],
   "source": [
    "# Experiment configurations\n",
    "experiment_configs = {\n",
    "    'learning_rates': [0.001, 0.0001],\n",
    "    'batch_sizes': [32, 16] if gpu_available else [16, 8],\n",
    "    'complexities': ['simple', 'medium'],\n",
    "    'dropout_rates': [0.25, 0.3]\n",
    "}\n",
    "\n",
    "# Adjust experiment size based on GPU availability\n",
    "if gpu_available:\n",
    "    experiment_size = min(2000, X_train_images.shape[0])\n",
    "    val_size = min(500, X_val_images.shape[0])\n",
    "else:\n",
    "    experiment_size = min(1000, X_train_images.shape[0])\n",
    "    val_size = min(200, X_val_images.shape[0])\n",
    "\n",
    "# Create experiment datasets\n",
    "X_train_exp = X_train_images[:experiment_size]\n",
    "X_train_landmarks_exp = X_train_landmarks[:experiment_size]\n",
    "y_train_exp = y_train_onehot[:experiment_size]\n",
    "\n",
    "X_val_exp = X_val_images[:val_size]\n",
    "X_val_landmarks_exp = X_val_landmarks[:val_size]\n",
    "y_val_exp = y_val_onehot[:val_size]\n",
    "\n",
    "print(f\"üß™ EXPERIMENT SETUP:\")\n",
    "print(f\"   Training samples: {experiment_size:,}\")\n",
    "print(f\"   Validation samples: {val_size:,}\")\n",
    "print(f\"   Device: {'GPU' if gpu_available else 'CPU'}\")\n",
    "print(f\"   Configs to test: {len(experiment_configs['learning_rates']) * len(experiment_configs['batch_sizes']) * len(experiment_configs['complexities'])}\")\n",
    "\n",
    "experiment_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üöÄ Run CNN Experiments\n",
    "\n",
    "Test berbagai konfigurasi CNN dengan epochs rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Experiment function defined!\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment(model_type, lr, batch_size, complexity, dropout_rate, epochs=5):\n",
    "    \"\"\"Run single experiment with given parameters\"\"\"\n",
    "    \n",
    "    print(f\"Testing {model_type}: LR={lr}, BS={batch_size}, Complexity={complexity}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Build model\n",
    "        if model_type == 'cnn':\n",
    "            model = build_cnn_model(X_train_exp.shape[1:], num_classes, complexity, dropout_rate)\n",
    "            X_train_model = X_train_exp\n",
    "            X_val_model = X_val_exp\n",
    "        else:  # landmark\n",
    "            model = build_landmark_model(X_train_landmarks_exp.shape[1], num_classes, complexity, dropout_rate)\n",
    "            X_train_model = X_train_landmarks_exp\n",
    "            X_val_model = X_val_landmarks_exp\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_model, y_train_exp,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val_model, y_val_exp),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get results\n",
    "        final_train_acc = history.history['accuracy'][-1]\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        overfitting = final_train_acc - final_val_acc\n",
    "        experiment_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'model_type': model_type,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'complexity': complexity,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'final_train_acc': final_train_acc,\n",
    "            'final_val_acc': final_val_acc,\n",
    "            'overfitting': overfitting,\n",
    "            'experiment_time': experiment_time,\n",
    "            'parameters': model.count_params(),\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Val Acc: {final_val_acc:.4f}, Time: {experiment_time:.1f}s\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'complexity': complexity,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Experiment function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è RUNNING CNN EXPERIMENTS...\n",
      "========================================\n",
      "Testing cnn: LR=0.001, BS=32, Complexity=simple\n",
      "   ‚úÖ Val Acc: 0.8282, Time: 23.3s\n",
      "Testing cnn: LR=0.001, BS=32, Complexity=medium\n"
     ]
    }
   ],
   "source": [
    "# Run CNN experiments\n",
    "print(\"üñºÔ∏è RUNNING CNN EXPERIMENTS...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cnn_results = []\n",
    "experiment_count = 0\n",
    "\n",
    "for lr in experiment_configs['learning_rates']:\n",
    "    for bs in experiment_configs['batch_sizes']:\n",
    "        for complexity in experiment_configs['complexities']:\n",
    "            experiment_count += 1\n",
    "            dropout = 0.25 if complexity == 'simple' else 0.3\n",
    "            epochs = 8 if gpu_available else 5\n",
    "            \n",
    "            result = run_single_experiment('cnn', lr, bs, complexity, dropout, epochs)\n",
    "            cnn_results.append(result)\n",
    "            experiment_results.append(result)\n",
    "\n",
    "successful_cnn = len([r for r in cnn_results if r['status'] == 'success'])\n",
    "print(f\"\\n‚úÖ CNN experiments completed: {successful_cnn}/{len(cnn_results)} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Landmark experiments\n",
    "print(\"\\nüéØ RUNNING LANDMARK EXPERIMENTS...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "landmark_results = []\n",
    "\n",
    "for lr in experiment_configs['learning_rates']:\n",
    "    for bs in experiment_configs['batch_sizes']:\n",
    "        for complexity in experiment_configs['complexities']:\n",
    "            dropout = 0.3 if complexity == 'simple' else 0.4\n",
    "            epochs = 10 if gpu_available else 6\n",
    "            \n",
    "            result = run_single_experiment('landmark', lr, bs, complexity, dropout, epochs)\n",
    "            landmark_results.append(result)\n",
    "            experiment_results.append(result)\n",
    "\n",
    "successful_landmark = len([r for r in landmark_results if r['status'] == 'success'])\n",
    "print(f\"\\n‚úÖ Landmark experiments completed: {successful_landmark}/{len(landmark_results)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìä Analyze Results\n",
    "\n",
    "Analisis hasil eksperimen untuk menemukan hyperparameter terbaik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "successful_results = [r for r in experiment_results if r['status'] == 'success']\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    df_results = pd.DataFrame(successful_results)\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(successful_results)} successful experiments...\")\n",
    "    \n",
    "    # Best results per model type\n",
    "    print(\"\\nüèÜ BEST RESULTS PER MODEL TYPE:\")\n",
    "    for model_type in df_results['model_type'].unique():\n",
    "        model_results = df_results[df_results['model_type'] == model_type]\n",
    "        best_idx = model_results['final_val_acc'].idxmax()\n",
    "        best = model_results.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{model_type.upper()}:\")\n",
    "        print(f\"   Best Val Acc: {best['final_val_acc']:.4f}\")\n",
    "        print(f\"   Learning Rate: {best['learning_rate']}\")\n",
    "        print(f\"   Batch Size: {best['batch_size']}\")\n",
    "        print(f\"   Complexity: {best['complexity']}\")\n",
    "        print(f\"   Overfitting: {best['overfitting']:.4f}\")\n",
    "    \n",
    "    # Top 3 overall\n",
    "    print(\"\\nüåü TOP 3 OVERALL RESULTS:\")\n",
    "    top_3 = df_results.nlargest(3, 'final_val_acc')\n",
    "    for idx, row in top_3.iterrows():\n",
    "        print(f\"{row['model_type']}: Val Acc {row['final_val_acc']:.4f}, LR {row['learning_rate']}, BS {row['batch_size']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful experiments to analyze!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if len(successful_results) > 0 and len(df_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Validation accuracy by learning rate\n",
    "    df_results.boxplot(column='final_val_acc', by='learning_rate', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Validation Accuracy by Learning Rate')\n",
    "    \n",
    "    # Validation accuracy by batch size\n",
    "    df_results.boxplot(column='final_val_acc', by='batch_size', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Validation Accuracy by Batch Size')\n",
    "    \n",
    "    # Overfitting by model type\n",
    "    df_results.boxplot(column='overfitting', by='model_type', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Overfitting by Model Type')\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Training time by complexity\n",
    "    df_results.boxplot(column='experiment_time', by='complexity', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Training Time by Complexity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üíæ Save Results & Recommendations\n",
    "\n",
    "Simpan hasil experiment dan berikan rekomendasi untuk training penuh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and recommendations\n",
    "if len(successful_results) > 0:\n",
    "    # Create recommendations\n",
    "    recommendations = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'experiment_summary': {\n",
    "            'total_experiments': len(experiment_results),\n",
    "            'successful_experiments': len(successful_results),\n",
    "            'gpu_used': gpu_available\n",
    "        },\n",
    "        'best_configurations': {}\n",
    "    }\n",
    "    \n",
    "    # Best config for each model type\n",
    "    for model_type in df_results['model_type'].unique():\n",
    "        model_results = df_results[df_results['model_type'] == model_type]\n",
    "        best = model_results.loc[model_results['final_val_acc'].idxmax()]\n",
    "        \n",
    "        recommendations['best_configurations'][model_type] = {\n",
    "            'learning_rate': best['learning_rate'],\n",
    "            'batch_size': int(best['batch_size']),\n",
    "            'complexity': best['complexity'],\n",
    "            'dropout_rate': best['dropout_rate'],\n",
    "            'expected_val_acc': best['final_val_acc']\n",
    "        }\n",
    "    \n",
    "    # Save to file\n",
    "    with open('experiment_results.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'detailed_results': experiment_results,\n",
    "            'recommendations': recommendations\n",
    "        }, f)\n",
    "    \n",
    "    print(\"üíæ Results saved to 'experiment_results.pkl'\")\n",
    "    \n",
    "    # Display final recommendations\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéØ FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for model_type, config in recommendations['best_configurations'].items():\n",
    "        print(f\"\\n{model_type.upper()} MODEL:\")\n",
    "        print(f\"   Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"   Batch Size: {config['batch_size']}\")\n",
    "        print(f\"   Complexity: {config['complexity']}\")\n",
    "        print(f\"   Dropout Rate: {config['dropout_rate']}\")\n",
    "        print(f\"   Expected Val Acc: {config['expected_val_acc']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìã NEXT STEPS:\")\n",
    "    print(\"   1. Update training scripts dengan parameter di atas\")\n",
    "    print(\"   2. Jalankan full training dengan early stopping\")\n",
    "    print(\"   3. Monitor training progress closely\")\n",
    "    print(\"   4. Test fusion strategies setelah individual models selesai\")\n",
    "    \n",
    "    print(\"\\nüöÄ READY FOR FULL TRAINING!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful experiments to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
