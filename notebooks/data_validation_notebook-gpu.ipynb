{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation & Quick Model Test\n",
    "\n",
    "**Tujuan notebook ini:**\n",
    "1. Validasi loading data dari semua sumber\n",
    "2. Cek konsistensi data multimodal (images + landmarks)\n",
    "3. Analisis distribusi kelas emosi\n",
    "4. Quick test model compilation dan training\n",
    "5. Estimasi waktu training\n",
    "6. Identifikasi potential issues sebelum training penuh\n",
    "\n",
    "**‚ö†Ô∏è JALANKAN NOTEBOOK INI DULU SEBELUM TRAINING PRODUCTION!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Setup TensorFlow for AMD GPU (DirectML)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTensorFlow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Try to setup DirectML for AMD GPU\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Setup TensorFlow for AMD GPU (DirectML)\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Try to setup DirectML for AMD GPU\n",
    "try:\n",
    "    # For AMD GPUs on Windows, try DirectML\n",
    "    from tensorflow.python.client import device_lib\n",
    "    \n",
    "    # List available devices\n",
    "    devices = device_lib.list_local_devices()\n",
    "    gpu_devices = [d for d in devices if d.device_type == 'GPU']\n",
    "    \n",
    "    if gpu_devices:\n",
    "        print(f\"\\nüéÆ Found {len(gpu_devices)} GPU(s):\")\n",
    "        for gpu in gpu_devices:\n",
    "            print(f\"   - {gpu.name}: {gpu.physical_device_desc}\")\n",
    "        \n",
    "        # Configure GPU memory growth to prevent OOM\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(\"‚úÖ GPU memory growth enabled\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"‚ö†Ô∏è GPU setup warning: {e}\")\n",
    "        \n",
    "        # Test GPU availability\n",
    "        print(f\"\\nüîç GPU Test:\")\n",
    "        print(f\"   - GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "        print(f\"   - Built with GPU support: {tf.test.is_built_with_gpu_support()}\")\n",
    "        \n",
    "        # Set default device for GPU operations\n",
    "        tf.config.set_soft_device_placement(True)\n",
    "        print(\"‚úÖ GPU setup completed successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU detected. Using CPU mode.\")\n",
    "        print(\"üí° For AMD GPU support, install: pip install tensorflow-directml\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GPU setup failed: {e}\")\n",
    "    print(\"üí° Falling back to CPU mode\")\n",
    "    print(\"üí° For AMD RX 6600 LE support, install: pip install tensorflow-directml\")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"\\n‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîç Data Loading Validation\n",
    "\n",
    "Pertama kita validasi apakah semua data bisa di-load dengan benar dan path-nya konsisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path - SESUAIKAN DENGAN PATH ANDA\n",
    "BASE_PATH = 'D:/research/2025_iris_taufik/MultimodalEmoLearn-CNN-LSTM/data/'\n",
    "\n",
    "print(\"=== DATA LOADING VALIDATION ===\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "\n",
    "# Check if directories exist\n",
    "image_path = os.path.join(BASE_PATH, 'images')\n",
    "landmark_path = os.path.join(BASE_PATH, 'landmarks')\n",
    "\n",
    "print(f\"Image path exists: {os.path.exists(image_path)}\")\n",
    "print(f\"Landmark path exists: {os.path.exists(landmark_path)}\")\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    print(f\"Files in image path: {os.listdir(image_path)}\")\n",
    "if os.path.exists(landmark_path):\n",
    "    print(f\"Files in landmark path: {os.listdir(landmark_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading all data files\n",
    "data_files = {\n",
    "    'X_train_images': os.path.join(BASE_PATH, 'images/X_train_images.npy'),\n",
    "    'X_val_images': os.path.join(BASE_PATH, 'images/X_val_images.npy'),\n",
    "    'X_test_images': os.path.join(BASE_PATH, 'images/X_test_images.npy'),\n",
    "    'y_train_images': os.path.join(BASE_PATH, 'images/y_train_images.npy'),\n",
    "    'y_val_images': os.path.join(BASE_PATH, 'images/y_val_images.npy'),\n",
    "    'y_test_images': os.path.join(BASE_PATH, 'images/y_test_images.npy'),\n",
    "    'X_train_landmarks': os.path.join(BASE_PATH, 'landmarks/X_train_landmarks.npy'),\n",
    "    'X_val_landmarks': os.path.join(BASE_PATH, 'landmarks/X_val_landmarks.npy'),\n",
    "    'X_test_landmarks': os.path.join(BASE_PATH, 'landmarks/X_test_landmarks.npy')\n",
    "}\n",
    "\n",
    "loaded_data = {}\n",
    "loading_status = {}\n",
    "\n",
    "for name, path in data_files.items():\n",
    "    try:\n",
    "        loaded_data[name] = np.load(path)\n",
    "        loading_status[name] = '‚úÖ Success'\n",
    "        print(f\"‚úÖ {name}: {loaded_data[name].shape}\")\n",
    "    except Exception as e:\n",
    "        loading_status[name] = f'‚ùå Error: {str(e)}'\n",
    "        print(f\"‚ùå {name}: {str(e)}\")\n",
    "\n",
    "print(\"\\n=== LOADING SUMMARY ===\")\n",
    "success_count = sum(1 for status in loading_status.values() if '‚úÖ' in status)\n",
    "print(f\"Successfully loaded: {success_count}/{len(data_files)} files\")\n",
    "\n",
    "if success_count < len(data_files):\n",
    "    print(\"\\n‚ö†Ô∏è BEBERAPA FILE TIDAK BISA DI-LOAD! Periksa path atau jalankan preprocessing dulu.\")\n",
    "else:\n",
    "    print(\"\\nüéâ SEMUA FILE BERHASIL DI-LOAD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìä Data Consistency Check\n",
    "\n",
    "Memastikan jumlah samples konsisten antara images, landmarks, dan labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success_count == len(data_files):\n",
    "    print(\"=== DATA CONSISTENCY CHECK ===\")\n",
    "    \n",
    "    # Extract data untuk kemudahan\n",
    "    X_train_images = loaded_data['X_train_images']\n",
    "    X_val_images = loaded_data['X_val_images']\n",
    "    X_test_images = loaded_data['X_test_images']\n",
    "    \n",
    "    X_train_landmarks = loaded_data['X_train_landmarks']\n",
    "    X_val_landmarks = loaded_data['X_val_landmarks']\n",
    "    X_test_landmarks = loaded_data['X_test_landmarks']\n",
    "    \n",
    "    y_train = loaded_data['y_train_images']\n",
    "    y_val = loaded_data['y_val_images']\n",
    "    y_test = loaded_data['y_test_images']\n",
    "    \n",
    "    # Check shapes\n",
    "    print(\"üìè SHAPES:\")\n",
    "    print(f\"Images - Train: {X_train_images.shape}, Val: {X_val_images.shape}, Test: {X_test_images.shape}\")\n",
    "    print(f\"Landmarks - Train: {X_train_landmarks.shape}, Val: {X_val_landmarks.shape}, Test: {X_test_landmarks.shape}\")\n",
    "    print(f\"Labels - Train: {y_train.shape}, Val: {y_val.shape}, Test: {y_test.shape}\")\n",
    "    \n",
    "    # Check consistency\n",
    "    consistency_checks = [\n",
    "        (\"Training samples\", X_train_images.shape[0] == X_train_landmarks.shape[0] == y_train.shape[0]),\n",
    "        (\"Validation samples\", X_val_images.shape[0] == X_val_landmarks.shape[0] == y_val.shape[0]),\n",
    "        (\"Test samples\", X_test_images.shape[0] == X_test_landmarks.shape[0] == y_test.shape[0])\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç CONSISTENCY CHECKS:\")\n",
    "    all_consistent = True\n",
    "    for check_name, is_consistent in consistency_checks:\n",
    "        status = \"‚úÖ\" if is_consistent else \"‚ùå\"\n",
    "        print(f\"{status} {check_name}: {is_consistent}\")\n",
    "        if not is_consistent:\n",
    "            all_consistent = False\n",
    "    \n",
    "    if all_consistent:\n",
    "        print(\"\\nüéâ ALL DATA IS CONSISTENT!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è DATA INCONSISTENCY DETECTED! Fix sebelum training.\")\n",
    "        \n",
    "    # Data types\n",
    "    print(\"\\nüìã DATA TYPES:\")\n",
    "    print(f\"Images dtype: {X_train_images.dtype}\")\n",
    "    print(f\"Landmarks dtype: {X_train_landmarks.dtype}\")\n",
    "    print(f\"Labels dtype: {y_train.dtype}\")\n",
    "    print(f\"Labels type: {type(y_train[0])}\")\n",
    "    \n",
    "    # Value ranges\n",
    "    print(\"\\nüìà VALUE RANGES:\")\n",
    "    print(f\"Images: min={X_train_images.min():.3f}, max={X_train_images.max():.3f}\")\n",
    "    print(f\"Landmarks: min={X_train_landmarks.min():.3f}, max={X_train_landmarks.max():.3f}\")\n",
    "    print(f\"Sample labels: {y_train[:5]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping consistency check karena ada file yang tidak bisa di-load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üé≠ Class Distribution Analysis\n",
    "\n",
    "Analisis distribusi kelas emosi untuk memastikan data balanced dan tidak ada missing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success_count == len(data_files):\n",
    "    print(\"=== CLASS DISTRIBUTION ANALYSIS ===\")\n",
    "    \n",
    "    # Get unique labels\n",
    "    all_labels = np.concatenate([y_train, y_val, y_test])\n",
    "    unique_labels = np.unique(all_labels)\n",
    "    \n",
    "    print(f\"üè∑Ô∏è Unique labels: {unique_labels}\")\n",
    "    print(f\"üìä Number of classes: {len(unique_labels)}\")\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    reverse_label_map = {i: label for label, i in label_map.items()}\n",
    "    \n",
    "    print(f\"üî¢ Label mapping: {label_map}\")\n",
    "    \n",
    "    # Distribution per split\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    splits = [('Train', y_train), ('Validation', y_val), ('Test', y_test), ('Overall', all_labels)]\n",
    "    \n",
    "    for i, (split_name, labels) in enumerate(splits):\n",
    "        ax = axes[i//2, i%2]\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        \n",
    "        bars = ax.bar(range(len(unique)), counts, alpha=0.7)\n",
    "        ax.set_title(f'{split_name} Distribution (Total: {len(labels)})')\n",
    "        ax.set_xlabel('Emotion Class')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_xticks(range(len(unique)))\n",
    "        ax.set_xticklabels(unique, rotation=45, ha='right')\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                   str(count), ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    print(\"\\n‚öñÔ∏è CLASS BALANCE ANALYSIS:\")\n",
    "    train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "    min_count = train_counts.min()\n",
    "    max_count = train_counts.max()\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    \n",
    "    print(f\"Min class count: {min_count}\")\n",
    "    print(f\"Max class count: {max_count}\")\n",
    "    print(f\"Imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"‚ö†Ô∏è SIGNIFICANT CLASS IMBALANCE! Consider using class weights atau resampling.\")\n",
    "    elif imbalance_ratio > 2:\n",
    "        print(\"‚ö†Ô∏è Moderate class imbalance. Monitor training carefully.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Classes relatively balanced.\")\n",
    "        \n",
    "    # Store untuk use nanti\n",
    "    globals()['label_map'] = label_map\n",
    "    globals()['num_classes'] = len(unique_labels)\n",
    "    globals()['target_names'] = list(unique_labels)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping class distribution analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß™ Quick Model Test\n",
    "\n",
    "Test model compilation dan training dengan sample kecil untuk memastikan tidak ada error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success_count == len(data_files) and all_consistent:\n",
    "    print(\"=== QUICK MODEL TEST ===\")\n",
    "    \n",
    "    # Take small sample for quick test\n",
    "    sample_size = min(200, X_train_images.shape[0])  # Max 200 samples\n",
    "    \n",
    "    X_train_small = X_train_images[:sample_size]\n",
    "    X_train_landmarks_small = X_train_landmarks[:sample_size]\n",
    "    y_train_small = y_train[:sample_size]\n",
    "    \n",
    "    print(f\"üî¨ Testing with {sample_size} samples\")\n",
    "    print(f\"Sample shapes: Images{X_train_small.shape}, Landmarks{X_train_landmarks_small.shape}\")\n",
    "    \n",
    "    # Convert labels\n",
    "    y_train_num = np.array([label_map[label] for label in y_train_small])\n",
    "    y_train_onehot = to_categorical(y_train_num, num_classes)\n",
    "    \n",
    "    print(f\"Label conversion test: {y_train_small[:3]} -> {y_train_num[:3]}\")\n",
    "    print(f\"One-hot shape: {y_train_onehot.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping model test due to data issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple CNN with GPU optimization\n",
    "if 'X_train_small' in locals():\n",
    "    print(\"\\nüß† Testing Simple CNN with GPU...\")\n",
    "    \n",
    "    try:\n",
    "        # Use GPU-optimized batch size for RX 6600 LE\n",
    "        gpu_batch_size = 32 if len(tf.config.list_physical_devices('GPU')) > 0 else 16\n",
    "        \n",
    "        with tf.device('/GPU:0' if len(tf.config.list_physical_devices('GPU')) > 0 else '/CPU:0'):\n",
    "            simple_cnn = Sequential([\n",
    "                Conv2D(16, (3, 3), activation='relu', input_shape=X_train_small.shape[1:]),\n",
    "                MaxPooling2D((2, 2)),\n",
    "                Conv2D(32, (3, 3), activation='relu'),\n",
    "                MaxPooling2D((2, 2)),\n",
    "                Flatten(),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            simple_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            print(f\"‚úÖ Simple CNN compiled! Parameters: {simple_cnn.count_params():,}\")\n",
    "            \n",
    "            # Test training with timing\n",
    "            print(f\"üîÑ Training with batch size {gpu_batch_size} on {'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            history = simple_cnn.fit(\n",
    "                X_train_small, y_train_onehot, \n",
    "                epochs=2, \n",
    "                verbose=1, \n",
    "                batch_size=gpu_batch_size,\n",
    "                validation_split=0.2\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            print(f\"‚úÖ Training completed! Time: {training_time:.1f}s, Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "            \n",
    "            # Store timing info for later estimation\n",
    "            globals()['gpu_training_time'] = training_time\n",
    "            globals()['gpu_batch_size'] = gpu_batch_size\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CNN test failed: {str(e)}\")\n",
    "        print(\"üí° Trying CPU fallback...\")\n",
    "        try:\n",
    "            simple_cnn = Sequential([\n",
    "                Conv2D(16, (3, 3), activation='relu', input_shape=X_train_small.shape[1:]),\n",
    "                MaxPooling2D((2, 2)),\n",
    "                Flatten(),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            simple_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            history = simple_cnn.fit(X_train_small, y_train_onehot, epochs=1, verbose=0, batch_size=16)\n",
    "            print(f\"‚úÖ CPU fallback successful!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå CPU fallback also failed: {str(e2)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping CNN test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Simple Landmark Model\n",
    "if 'X_train_landmarks_small' in locals():\n",
    "    print(\"\\nüéØ Testing Simple Landmark Model...\")\n",
    "    \n",
    "    try:\n",
    "        simple_landmark = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(X_train_landmarks_small.shape[1],)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        simple_landmark.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(f\"‚úÖ Landmark model compiled! Parameters: {simple_landmark.count_params():,}\")\n",
    "        \n",
    "        # Test training\n",
    "        history = simple_landmark.fit(X_train_landmarks_small, y_train_onehot, epochs=2, verbose=1, batch_size=16)\n",
    "        print(f\"‚úÖ Training test completed! Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Landmark model test failed: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping landmark model test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Simple Fusion Model\n",
    "if 'X_train_small' in locals() and 'X_train_landmarks_small' in locals():\n",
    "    print(\"\\nüîó Testing Simple Fusion Model...\")\n",
    "    \n",
    "    try:\n",
    "        # Image branch\n",
    "        image_input = Input(shape=X_train_small.shape[1:])\n",
    "        x_img = Conv2D(16, (3, 3), activation='relu')(image_input)\n",
    "        x_img = MaxPooling2D((2, 2))(x_img)\n",
    "        x_img = Flatten()(x_img)\n",
    "        x_img = Dense(32, activation='relu')(x_img)\n",
    "        \n",
    "        # Landmark branch\n",
    "        landmark_input = Input(shape=(X_train_landmarks_small.shape[1],))\n",
    "        x_lm = Dense(64, activation='relu')(landmark_input)\n",
    "        x_lm = Dense(32, activation='relu')(x_lm)\n",
    "        \n",
    "        # Fusion\n",
    "        fusion = concatenate([x_img, x_lm])\n",
    "        x = Dense(64, activation='relu')(fusion)\n",
    "        output = Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        fusion_model = Model(inputs=[image_input, landmark_input], outputs=output)\n",
    "        fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        print(f\"‚úÖ Fusion model compiled! Parameters: {fusion_model.count_params():,}\")\n",
    "        \n",
    "        # Test training\n",
    "        history = fusion_model.fit([X_train_small, X_train_landmarks_small], y_train_onehot, \n",
    "                                 epochs=2, verbose=1, batch_size=16)\n",
    "        print(f\"‚úÖ Training test completed! Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fusion model test failed: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping fusion model test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚è±Ô∏è Training Time Estimation\n",
    "\n",
    "Estimasi waktu training untuk membantu planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'simple_cnn' in locals():\n",
    "    print(\"=== TRAINING TIME ESTIMATION FOR RX 6600 LE ===\")\n",
    "    \n",
    "    # Enhanced estimation for GPU vs CPU\n",
    "    test_samples = min(1000, X_train_images.shape[0])\n",
    "    device_type = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n",
    "    \n",
    "    print(f\"üî¨ Testing prediction time dengan {test_samples} samples on {device_type}...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        _ = simple_cnn.predict(X_train_images[:test_samples], verbose=0)\n",
    "        prediction_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate estimates\n",
    "        total_samples = X_train_images.shape[0]\n",
    "        time_per_sample = prediction_time / test_samples\n",
    "        \n",
    "        # Different multipliers for GPU vs CPU\n",
    "        if device_type == 'GPU':\n",
    "            # GPU training is typically 2-3x prediction time\n",
    "            training_multiplier = 2.5\n",
    "            batch_size_used = gpu_batch_size if 'gpu_batch_size' in locals() else 32\n",
    "        else:\n",
    "            # CPU training is typically 3-4x prediction time\n",
    "            training_multiplier = 3.5\n",
    "            batch_size_used = 16\n",
    "        \n",
    "        estimated_time_per_epoch = (time_per_sample * total_samples * training_multiplier) / 60  # dalam menit\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è TIME ESTIMATES FOR {device_type}:\")\n",
    "        print(f\"Prediction time per sample: {time_per_sample*1000:.2f} ms\")\n",
    "        print(f\"Recommended batch size: {batch_size_used}\")\n",
    "        print(f\"Estimated time per epoch: {estimated_time_per_epoch:.1f} minutes\")\n",
    "        \n",
    "        # Different estimates for different models\n",
    "        print(f\"\\nüìä ESTIMATED TRAINING TIMES:\")\n",
    "        models_estimates = {\n",
    "            'CNN Model (50 epochs)': estimated_time_per_epoch * 50 / 60,\n",
    "            'Landmark Model (50 epochs)': (estimated_time_per_epoch * 0.3) * 50 / 60,  # Landmark models are much faster\n",
    "            'Hybrid Fusion (50 epochs)': estimated_time_per_epoch * 1.5 * 50 / 60,  # Fusion models are more complex\n",
    "            'Late Fusion (weight search)': estimated_time_per_epoch * 0.1  # Very fast, just inference\n",
    "        }\n",
    "        \n",
    "        total_hours = 0\n",
    "        for model_name, hours in models_estimates.items():\n",
    "            print(f\"   - {model_name}: {hours:.1f} hours\")\n",
    "            if 'Late Fusion' not in model_name:\n",
    "                total_hours += hours\n",
    "        \n",
    "        print(f\"\\nüéØ TOTAL ESTIMATED TIME: {total_hours:.1f} hours\")\n",
    "        \n",
    "        # GPU vs CPU comparison\n",
    "        if device_type == 'GPU':\n",
    "            cpu_estimate = total_hours * 4  # GPU is typically 4x faster\n",
    "            print(f\"\\nüöÄ GPU ADVANTAGE:\")\n",
    "            print(f\"   - With RX 6600 LE: ~{total_hours:.1f} hours\")\n",
    "            print(f\"   - CPU only would take: ~{cpu_estimate:.1f} hours\")\n",
    "            print(f\"   - Time saved: ~{cpu_estimate - total_hours:.1f} hours ({((cpu_estimate - total_hours)/cpu_estimate*100):.0f}% faster)\")\n",
    "        \n",
    "        # Recommendations based on estimates\n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        if total_hours > 12:\n",
    "            print(\"   ‚ö†Ô∏è Very long training time predicted!\")\n",
    "            print(\"   - Consider reducing image resolution\")\n",
    "            print(\"   - Use smaller model complexity\")\n",
    "            print(\"   - Implement aggressive early stopping\")\n",
    "        elif total_hours > 6:\n",
    "            print(\"   ‚ö†Ô∏è Long training time. Plan accordingly.\")\n",
    "            print(\"   - Use early stopping with patience 10-15\")\n",
    "            print(\"   - Monitor training closely\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Reasonable training time expected\")\n",
    "        \n",
    "        if device_type == 'GPU':\n",
    "            print(f\"   - Optimal batch size for RX 6600 LE: {batch_size_used}\")\n",
    "            print(\"   - Monitor GPU memory usage during training\")\n",
    "        \n",
    "        # Store estimate untuk summary\n",
    "        globals()['estimated_time_per_epoch'] = estimated_time_per_epoch\n",
    "        globals()['total_training_hours'] = total_hours\n",
    "        globals()['device_type'] = device_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Time estimation failed: {e}\")\n",
    "        print(\"‚ö†Ô∏è Proceeding without time estimates\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot estimate training time - model test failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìã Final Summary & Recommendations\n",
    "\n",
    "Ringkasan hasil validasi dan rekomendasi untuk training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"üéØ FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Collect all validation results\n",
    "validation_results = {\n",
    "    'data_loading': success_count == len(data_files),\n",
    "    'data_consistency': 'all_consistent' in locals() and all_consistent,\n",
    "    'model_compilation': 'simple_cnn' in locals(),\n",
    "    'training_test': 'simple_cnn' in locals(),\n",
    "}\n",
    "\n",
    "print(\"\\nüìä VALIDATION STATUS:\")\n",
    "for check, status in validation_results.items():\n",
    "    icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{icon} {check.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "all_passed = all(validation_results.values())\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\nüéâ ALL VALIDATIONS PASSED!\")\n",
    "    print(\"\\nüìà DATASET INFO:\")\n",
    "    if 'num_classes' in locals():\n",
    "        print(f\"   - Classes: {num_classes}\")\n",
    "        print(f\"   - Training samples: {X_train_images.shape[0]:,}\")\n",
    "        print(f\"   - Validation samples: {X_val_images.shape[0]:,}\")\n",
    "        print(f\"   - Test samples: {X_test_images.shape[0]:,}\")\n",
    "        print(f\"   - Image shape: {X_train_images.shape[1:]}\")\n",
    "        print(f\"   - Landmark features: {X_train_landmarks.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ READY FOR TRAINING! Recommendations:\")\n",
    "    print(\"   1. Start dengan epochs rendah (10-20) untuk initial testing\")\n",
    "    print(\"   2. Use early stopping dengan patience 10-15\")\n",
    "    print(\"   3. Monitor validation accuracy closely\")\n",
    "    print(\"   4. Save best model berdasarkan validation accuracy\")\n",
    "    \n",
    "    if 'estimated_time_per_epoch' in locals():\n",
    "        print(f\"   5. Set realistic time expectations (~{estimated_time_per_epoch:.0f} min/epoch)\")\n",
    "    \n",
    "    if 'imbalance_ratio' in locals() and imbalance_ratio > 2:\n",
    "        print(\"   6. Consider using class weights due to imbalance\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è VALIDATION FAILED! Fix issues before training:\")\n",
    "    for check, status in validation_results.items():\n",
    "        if not status:\n",
    "            print(f\"   - Fix {check.replace('_', ' ')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation results for reference\n",
    "if all_passed:\n",
    "    validation_summary = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'data_shapes': {\n",
    "            'images': {\n",
    "                'train': X_train_images.shape,\n",
    "                'val': X_val_images.shape, \n",
    "                'test': X_test_images.shape\n",
    "            },\n",
    "            'landmarks': {\n",
    "                'train': X_train_landmarks.shape,\n",
    "                'val': X_val_landmarks.shape,\n",
    "                'test': X_test_landmarks.shape\n",
    "            }\n",
    "        },\n",
    "        'label_info': {\n",
    "            'num_classes': num_classes,\n",
    "            'label_map': label_map,\n",
    "            'target_names': target_names\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'train': dict(zip(*np.unique(y_train, return_counts=True))),\n",
    "            'val': dict(zip(*np.unique(y_val, return_counts=True))),\n",
    "            'test': dict(zip(*np.unique(y_test, return_counts=True)))\n",
    "        },\n",
    "        'validation_status': validation_results\n",
    "    }\n",
    "    \n",
    "    if 'estimated_time_per_epoch' in locals():\n",
    "        validation_summary['training_estimates'] = {\n",
    "            'minutes_per_epoch': estimated_time_per_epoch,\n",
    "            'hours_for_50_epochs': estimated_time_per_epoch * 50 / 60,\n",
    "            'hours_for_100_epochs': estimated_time_per_epoch * 100 / 60\n",
    "        }\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = 'validation_results.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(validation_summary, f)\n",
    "    \n",
    "    print(f\"\\nüíæ Validation results saved to: {output_file}\")\n",
    "    print(\"\\nüöÄ You can now proceed with training your models!\")\n",
    "    print(\"\\nüìù Recommended next steps:\")\n",
    "    print(\"   1. Run train_cnn.py first\")\n",
    "    print(\"   2. Then train_lstm.py (landmark model)\")\n",
    "    print(\"   3. Then train_late_fusion.py\")\n",
    "    print(\"   4. Finally train_hybrid_fusion.py\")\n",
    "    print(\"   5. Run comparation.py untuk final comparison\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Cannot save results - validation failed.\")\n",
    "    print(\"Fix the issues above and re-run this notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
