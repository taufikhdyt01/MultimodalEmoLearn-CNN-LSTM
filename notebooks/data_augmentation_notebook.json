{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Augmentation and Oversampling for Imbalanced Emotion Dataset\n",
        "\n",
        "This notebook handles data imbalance using appropriate techniques for multimodal emotion recognition:\n",
        "1. **Image Augmentation** for CNN data (rotation, brightness, noise, etc.)\n",
        "2. **SMOTE** for landmark data (LSTM/FCNN)\n",
        "3. **Hybrid approaches** combining both techniques\n",
        "4. **Evaluation and export** of balanced datasets\n",
        "\n",
        "## Configuration\n",
        "Update these paths to match your preprocessed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these paths\n",
        "DATA_DIR = \"D:/Preprocessing/Final_Dataset\"  # Where your preprocessed data is stored\n",
        "CNN_DATA_DIR = \"D:/Preprocessing/Final_Dataset/CNN_Data\"\n",
        "LSTM_DATA_DIR = \"D:/Preprocessing/Final_Dataset/LSTM_Data\"\n",
        "FCNN_DATA_DIR = \"D:/Preprocessing/Final_Dataset/FCNN_Data\"\n",
        "OUTPUT_DIR = \"D:/Preprocessing/Balanced_Dataset\"  # Where to save augmented data\n",
        "\n",
        "# Augmentation parameters\n",
        "TARGET_SAMPLES_PER_CLASS = 1000  # Target number of samples per emotion class\n",
        "MIN_SAMPLES_THRESHOLD = 100      # Classes below this will be oversampled\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Image augmentation parameters\n",
        "IMG_AUG_PARAMS = {\n",
        "    'rotation_range': 15,\n",
        "    'width_shift_range': 0.1,\n",
        "    'height_shift_range': 0.1,\n",
        "    'brightness_range': [0.8, 1.2],\n",
        "    'zoom_range': 0.1,\n",
        "    'horizontal_flip': True,\n",
        "    'noise_factor': 0.1\n",
        "}\n",
        "\n",
        "# SMOTE parameters for landmarks\n",
        "SMOTE_PARAMS = {\n",
        "    'k_neighbors': 5,\n",
        "    'random_state': RANDOM_STATE\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Image augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# SMOTE for landmark data\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "for subdir in ['CNN_Data', 'LSTM_Data', 'FCNN_Data', 'Analysis']:\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, subdir), exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported and directories created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Analyze Data Imbalance\n",
        "\n",
        "First, let's analyze the current distribution of emotions in our dataset to understand the imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_data_distribution(data_dir):\n",
        "    \"\"\"Analyze emotion distribution across all splits.\"\"\"\n",
        "    print(\"Analyzing data distribution...\")\n",
        "    \n",
        "    splits = ['train', 'val', 'test']\n",
        "    all_distributions = {}\n",
        "    \n",
        "    for split in splits:\n",
        "        # Check if we have landmark data\n",
        "        landmark_file = os.path.join(data_dir, 'LSTM_Data', f'y_{split}.npy')\n",
        "        if os.path.exists(landmark_file):\n",
        "            y = np.load(landmark_file)\n",
        "            distribution = Counter(y)\n",
        "            all_distributions[split] = distribution\n",
        "            \n",
        "            print(f\"\\n{split.upper()} SET:\")\n",
        "            total = len(y)\n",
        "            for emotion, count in sorted(distribution.items()):\n",
        "                percentage = (count / total) * 100\n",
        "                print(f\"  {emotion}: {count} ({percentage:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"Warning: No data found for {split} set\")\n",
        "    \n",
        "    return all_distributions\n",
        "\n",
        "def visualize_distribution(distributions):\n",
        "    \"\"\"Create visualization of emotion distributions.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Emotion Distribution Analysis', fontsize=16)\n",
        "    \n",
        "    # Plot individual splits\n",
        "    for idx, (split, dist) in enumerate(distributions.items()):\n",
        "        if idx < 3:  # train, val, test\n",
        "            row, col = idx // 2, idx % 2\n",
        "            emotions = list(dist.keys())\n",
        "            counts = list(dist.values())\n",
        "            \n",
        "            axes[row, col].bar(emotions, counts, color=plt.cm.Set3(np.linspace(0, 1, len(emotions))))\n",
        "            axes[row, col].set_title(f'{split.title()} Set')\n",
        "            axes[row, col].set_ylabel('Number of Samples')\n",
        "            axes[row, col].tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Add count labels on bars\n",
        "            for i, count in enumerate(counts):\n",
        "                axes[row, col].text(i, count + max(counts)*0.01, str(count), \n",
        "                                   ha='center', va='bottom')\n",
        "    \n",
        "    # Combined distribution\n",
        "    combined_dist = Counter()\n",
        "    for dist in distributions.values():\n",
        "        combined_dist.update(dist)\n",
        "    \n",
        "    emotions = list(combined_dist.keys())\n",
        "    counts = list(combined_dist.values())\n",
        "    \n",
        "    axes[1, 1].bar(emotions, counts, color=plt.cm.Set3(np.linspace(0, 1, len(emotions))))\n",
        "    axes[1, 1].set_title('Combined Distribution')\n",
        "    axes[1, 1].set_ylabel('Number of Samples')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    for i, count in enumerate(counts):\n",
        "        axes[1, 1].text(i, count + max(counts)*0.01, str(count), \n",
        "                       ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'Analysis', 'original_distribution.png'), \n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return combined_dist\n",
        "\n",
        "def identify_imbalance(combined_dist, threshold=MIN_SAMPLES_THRESHOLD):\n",
        "    \"\"\"Identify which classes need oversampling.\"\"\"\n",
        "    print(f\"\\nImbalance Analysis (threshold: {threshold} samples):\")\n",
        "    \n",
        "    minority_classes = []\n",
        "    majority_classes = []\n",
        "    \n",
        "    max_count = max(combined_dist.values())\n",
        "    min_count = min(combined_dist.values())\n",
        "    \n",
        "    print(f\"\\nClass distribution:\")\n",
        "    for emotion, count in sorted(combined_dist.items(), key=lambda x: x[1], reverse=True):\n",
        "        ratio = count / max_count\n",
        "        status = \"MINORITY\" if count < threshold else \"OK\"\n",
        "        print(f\"  {emotion}: {count} samples (ratio: {ratio:.2f}) [{status}]\")\n",
        "        \n",
        "        if count < threshold:\n",
        "            minority_classes.append(emotion)\n",
        "        else:\n",
        "            majority_classes.append(emotion)\n",
        "    \n",
        "    print(f\"\\nImbalance ratio: {max_count/min_count:.1f}:1\")\n",
        "    print(f\"Classes needing oversampling: {minority_classes}\")\n",
        "    \n",
        "    return minority_classes, majority_classes\n",
        "\n",
        "# Run analysis\n",
        "distributions = analyze_data_distribution(DATA_DIR)\n",
        "combined_dist = visualize_distribution(distributions)\n",
        "minority_classes, majority_classes = identify_imbalance(combined_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Image Augmentation for CNN Data\n",
        "\n",
        "Apply image augmentation techniques to balance the CNN dataset by generating synthetic images for minority classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_image_augmentor():\n",
        "    \"\"\"Create image data generator for augmentation.\"\"\"\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=IMG_AUG_PARAMS['rotation_range'],\n",
        "        width_shift_range=IMG_AUG_PARAMS['width_shift_range'],\n",
        "        height_shift_range=IMG_AUG_PARAMS['height_shift_range'],\n",
        "        brightness_range=IMG_AUG_PARAMS['brightness_range'],\n",
        "        zoom_range=IMG_AUG_PARAMS['zoom_range'],\n",
        "        horizontal_flip=IMG_AUG_PARAMS['horizontal_flip'],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    return datagen\n",
        "\n",
        "def add_noise_augmentation(image, noise_factor=0.1):\n",
        "    \"\"\"Add random noise to image.\"\"\"\n",
        "    noise = np.random.normal(0, noise_factor, image.shape)\n",
        "    noisy_image = image + noise\n",
        "    return np.clip(noisy_image, 0, 1)\n",
        "\n",
        "def augment_cnn_data(cnn_data_dir, output_dir, minority_classes, target_samples):\n",
        "    \"\"\"Augment CNN data for minority classes.\"\"\"\n",
        "    print(\"\\nAugmenting CNN data...\")\n",
        "    \n",
        "    # Create augmentor\n",
        "    datagen = create_image_augmentor()\n",
        "    \n",
        "    for split in ['train']:  # Usually only augment training data\n",
        "        print(f\"\\nProcessing {split} set...\")\n",
        "        \n",
        "        # Load data\n",
        "        X_file = os.path.join(cnn_data_dir, f'X_{split}_images.npy')\n",
        "        y_file = os.path.join(cnn_data_dir, f'y_{split}_images.npy')\n",
        "        \n",
        "        if not (os.path.exists(X_file) and os.path.exists(y_file)):\n",
        "            print(f\"Warning: CNN data not found for {split} set\")\n",
        "            continue\n",
        "        \n",
        "        X = np.load(X_file)\n",
        "        y = np.load(y_file)\n",
        "        \n",
        "        print(f\"Original data shape: X={X.shape}, y={y.shape}\")\n",
        "        \n",
        "        # Create output directory for augmented images\n",
        "        aug_img_dir = os.path.join(output_dir, 'CNN_Data', f'{split}_images_augmented')\n",
        "        os.makedirs(aug_img_dir, exist_ok=True)\n",
        "        \n",
        "        # Prepare lists for augmented data\n",
        "        X_augmented = []\n",
        "        y_augmented = []\n",
        "        \n",
        "        # Add original data\n",
        "        for i in range(len(X)):\n",
        "            X_augmented.append(X[i])\n",
        "            y_augmented.append(y[i])\n",
        "        \n",
        "        # Augment minority classes\n",
        "        for emotion in minority_classes:\n",
        "            # Get indices for this emotion\n",
        "            emotion_indices = np.where(y == emotion)[0]\n",
        "            current_count = len(emotion_indices)\n",
        "            \n",
        "            if current_count == 0:\n",
        "                print(f\"  No samples found for {emotion} in {split} set\")\n",
        "                continue\n",
        "            \n",
        "            needed_samples = target_samples - current_count\n",
        "            if needed_samples <= 0:\n",
        "                print(f\"  {emotion}: {current_count} samples (no augmentation needed)\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"  {emotion}: {current_count} -> {target_samples} samples (generating {needed_samples})\")\n",
        "            \n",
        "            # Create emotion-specific directory\n",
        "            emotion_dir = os.path.join(aug_img_dir, emotion)\n",
        "            os.makedirs(emotion_dir, exist_ok=True)\n",
        "            \n",
        "            # Get original images for this emotion\n",
        "            emotion_images = X[emotion_indices]\n",
        "            \n",
        "            # Generate augmented samples\n",
        "            generated = 0\n",
        "            while generated < needed_samples:\n",
        "                # Randomly select an original image\n",
        "                idx = np.random.choice(len(emotion_images))\n",
        "                original_img = emotion_images[idx]\n",
        "                \n",
        "                # Apply augmentation\n",
        "                img_array = img_to_array(original_img)\n",
        "                img_array = img_array.reshape((1,) + img_array.shape)\n",
        "                \n",
        "                # Generate augmented image\n",
        "                aug_iter = datagen.flow(img_array, batch_size=1)\n",
        "                augmented_img = next(aug_iter)[0]\n",
        "                \n",
        "                # Optionally add noise\n",
        "                if np.random.random() < 0.3:  # 30% chance to add noise\n",
        "                    augmented_img = add_noise_augmentation(augmented_img, IMG_AUG_PARAMS['noise_factor'])\n",
        "                \n",
        "                # Add to dataset\n",
        "                X_augmented.append(augmented_img)\n",
        "                y_augmented.append(emotion)\n",
        "                \n",
        "                # Save augmented image\n",
        "                img_filename = f\"aug_{emotion}_{generated:04d}.jpg\"\n",
        "                img_path = os.path.join(emotion_dir, img_filename)\n",
        "                \n",
        "                # Convert back to image and save\n",
        "                img_to_save = (augmented_img * 255).astype(np.uint8)\n",
        "                cv2.imwrite(img_path, cv2.cvtColor(img_to_save, cv2.COLOR_RGB2BGR))\n",
        "                \n",
        "                generated += 1\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        X_final = np.array(X_augmented)\n",
        "        y_final = np.array(y_augmented)\n",
        "        \n",
        "        print(f\"\\nFinal augmented data shape: X={X_final.shape}, y={y_final.shape}\")\n",
        "        \n",
        "        # Save augmented data\n",
        "        np.save(os.path.join(output_dir, 'CNN_Data', f'X_{split}_augmented.npy'), X_final)\n",
        "        np.save(os.path.join(output_dir, 'CNN_Data', f'y_{split}_augmented.npy'), y_final)\n",
        "        \n",
        "        # Copy validation and test data (usually not augmented)\n",
        "        for other_split in ['val', 'test']:\n",
        "            X_other = os.path.join(cnn_data_dir, f'X_{other_split}_images.npy')\n",
        "            y_other = os.path.join(cnn_data_dir, f'y_{other_split}_images.npy')\n",
        "            \n",
        "            if os.path.exists(X_other) and os.path.exists(y_other):\n",
        "                shutil.copy2(X_other, os.path.join(output_dir, 'CNN_Data', f'X_{other_split}_augmented.npy'))\n",
        "                shutil.copy2(y_other, os.path.join(output_dir, 'CNN_Data', f'y_{other_split}_augmented.npy'))\n",
        "        \n",
        "        # Print final distribution\n",
        "        final_dist = Counter(y_final)\n",
        "        print(f\"\\nFinal emotion distribution in {split} set:\")\n",
        "        for emotion, count in sorted(final_dist.items()):\n",
        "            print(f\"  {emotion}: {count}\")\n",
        "\n",
        "# Run CNN data augmentation\n",
        "if minority_classes:\n",
        "    augment_cnn_data(CNN_DATA_DIR, OUTPUT_DIR, minority_classes, TARGET_SAMPLES_PER_CLASS)\n",
        "else:\n",
        "    print(\"No minority classes detected for CNN augmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: SMOTE for Landmark Data (LSTM/FCNN)\n",
        "\n",
        "Apply SMOTE (Synthetic Minority Oversampling Technique) to balance landmark data for LSTM and FCNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_smote_to_landmarks(data_dir, output_dir, data_type='LSTM', sampling_strategy='auto'):\n",
        "    \"\"\"Apply SMOTE to landmark data.\"\"\"\n",
        "    print(f\"\\nApplying SMOTE to {data_type} data...\")\n",
        "    \n",
        "    input_dir = os.path.join(data_dir, f'{data_type}_Data')\n",
        "    out_dir = os.path.join(output_dir, f'{data_type}_Data')\n",
        "    \n",
        "    for split in ['train']:  # Usually only oversample training data\n",
        "        print(f\"\\nProcessing {split} set...\")\n",
        "        \n",
        "        # Load data\n",
        "        X_file = os.path.join(input_dir, f'X_{split}_landmarks.npy')\n",
        "        y_file = os.path.join(input_dir, f'y_{split}.npy')\n",
        "        \n",
        "        if not (os.path.exists(X_file) and os.path.exists(y_file)):\n",
        "            print(f\"Warning: {data_type} data not found for {split} set\")\n",
        "            continue\n",
        "        \n",
        "        X = np.load(X_file)\n",
        "        y = np.load(y_file)\n",
        "        \n",
        "        print(f\"Original data shape: X={X.shape}, y={y.shape}\")\n",
        "        \n",
        "        # Original distribution\n",
        "        original_dist = Counter(y)\n",
        "        print(f\"Original distribution: {dict(original_dist)}\")\n",
        "        \n",
        "        # Check if we have enough samples for SMOTE\n",
        "        min_samples = min(original_dist.values())\n",
        "        if min_samples < 2:\n",
        "            print(f\"Warning: Some classes have < 2 samples. Removing them before SMOTE.\")\n",
        "            # Remove classes with < 2 samples\n",
        "            valid_classes = [cls for cls, count in original_dist.items() if count >= 2]\n",
        "            valid_indices = np.isin(y, valid_classes)\n",
        "            X = X[valid_indices]\n",
        "            y = y[valid_indices]\n",
        "            print(f\"Filtered data shape: X={X.shape}, y={y.shape}\")\n",
        "        \n",
        "        # Create sampling strategy\n",
        "        if sampling_strategy == 'auto':\n",
        "            # Auto-determine based on TARGET_SAMPLES_PER_CLASS\n",
        "            current_dist = Counter(y)\n",
        "            sampling_strategy_dict = {}\n",
        "            \n",
        "            for emotion, count in current_dist.items():\n",
        "                if count < TARGET_SAMPLES_PER_CLASS:\n",
        "                    sampling_strategy_dict[emotion] = TARGET_SAMPLES_PER_CLASS\n",
        "            \n",
        "            if not sampling_strategy_dict:\n",
        "                print(f\"No oversampling needed for {split} set\")\n",
        "                # Just copy the original data\n",
        "                np.save(os.path.join(out_dir, f'X_{split}_smote.npy'), X)\n",
        "                np.save(os.path.join(out_dir, f'y_{split}_smote.npy'), y)\n",
        "                continue\n",
        "            \n",
        "            sampling_strategy = sampling_strategy_dict\n",
        "        \n",
        "        print(f\"Sampling strategy: {sampling_strategy}\")\n",
        "        \n",
        "        # Try different SMOTE variants\n",
        "        smote_variants = [\n",
        "            ('SMOTE', SMOTE(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE, k_neighbors=min(5, min_samples-1))),\n",
        "            ('BorderlineSMOTE', BorderlineSMOTE(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE, k_neighbors=min(5, min_samples-1))),\n",
        "            ('ADASYN', ADASYN(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE, n_neighbors=min(5, min_samples-1)))\n",
        "        ]\n",
        "        \n",
        "        best_variant = None\n",
        "        best_X = None\n",
        "        best_y = None\n",
        "        \n",
        "        for variant_name, smote in smote_variants:\n",
        "            try:\n",
        "                print(f\"  Trying {variant_name}...\")\n",
        "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "                \n",
        "                # Check results\n",
        "                new_dist = Counter(y_resampled)\n",
        "                print(f\"    Success! New distribution: {dict(new_dist)}\")\n",
        "                \n",
        "                best_variant = variant_name\n",
        "                best_X = X_resampled\n",
        "                best_y = y_resampled\n",
        "                break  # Use first successful variant\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    Failed: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        if best_variant is None:\n",
        "            print(f\"All SMOTE variants failed for {split} set. Using original data.\")\n",
        "            best_X, best_y = X, y\n",
        "        else:\n",
        "            print(f\"Using {best_variant} for {split} set\")\n",
        "        \n",
        "        # Save results\n",
        "        np.save(os.path.join(out_dir, f'X_{split}_smote.npy'), best_X)\n",
        "        np.save(os.path.join(out_dir, f'y_{split}_smote.npy'), best_y)\n",
        "        \n",
        "        print(f\"Final data shape: X={best_X.shape}, y={best_y.shape}\")\n",
        "        \n",
        "        # Copy metadata if exists\n",
        "        metadata_file = os.path.join(input_dir, f'{split}_metadata.csv')\n",
        "        if os.path.exists(metadata_file):\n",
        "            # For SMOTE data, we can't directly map metadata since we have synthetic samples\n",
        "            # Create a simplified metadata file\n",
        "            original_meta = pd.read_csv(metadata_file)\n",
        "            \n",
        "            # Create metadata for original + synthetic samples\n",
        "            new_metadata = []\n",
        "            \n",
        "            # Add original metadata\n",
        "            for i in range(len(y)):\n",
        "                if i < len(original_meta):\n",
        "                    row = original_meta.iloc[i].to_dict()\n",
        "                    row['is_synthetic'] = False\n",
        "                    new_metadata.append(row)\n",
        "            \n",
        "            # Add synthetic sample metadata\n",
        "            for i in range(len(y), len(best_y)):\n",
        "                synthetic_row = {\n",
        "                    'user_id': 'synthetic',\n",
        "                    'timestamp': 'synthetic',\n",
        "                    'Confidence_Score': 1.0,\n",
        "                    'is_synthetic': True\n",
        "                }\n",
        "                if 'Challenge' in original_meta.columns:\n",
        "                    synthetic_row['Challenge'] = 'synthetic'\n",
        "                new_metadata.append(synthetic_row)\n",
        "            \n",
        "            # Save new metadata\n",
        "            new_meta_df = pd.DataFrame(new_metadata)\n",
        "            new_meta_df.to_csv(os.path.join(out_dir, f'{split}_metadata_smote.csv'), index=False)\n",
        "    \n",
        "    # Copy validation and test data (usually not oversampled)\n",
        "    for other_split in ['val', 'test']:\n",
        "        for file_type in ['landmarks', '']:\n",
        "            if file_type:\n",
        "                src_X = os.path.join(input_dir, f'X_{other_split}_{file_type}.npy')\n",
        "                dst_X = os.path.join(out_dir, f'X_{other_split}_smote.npy')\n",
        "            else:\n",
        "                src_X = os.path.join(input_dir, f'y_{other_split}.npy')\n",
        "                dst_X = os.path.join(out_dir, f'y_{other_split}_smote.npy')\n",
        "            \n",
        "            if os.path.exists(src_X):\n",
        "                shutil.copy2(src_X, dst_X)\n",
        "        \n",
        "        # Copy metadata\n",
        "        meta_src = os.path.join(input_dir, f'{other_split}_metadata.csv')\n",
        "        meta_dst = os.path.join(out_dir, f'{other_split}_metadata_smote.csv')\n",
        "        if os.path.exists(meta_src):\n",
        "            shutil.copy2(meta_src, meta_dst)\n",
        "\n",
        "# Apply SMOTE to both LSTM and FCNN data\n",
        "for data_type in ['LSTM', 'FCNN']:\n",
        "    data_dir_path = os.path.join(DATA_DIR, f'{data_type}_Data')\n",
        "    if os.path.exists(data_dir_path):\n",
        "        apply_smote_to_landmarks(DATA_DIR, OUTPUT_DIR, data_type)\n",
        "    else:\n",
        "        print(f\"Warning: {data_type} data directory not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Advanced Augmentation Techniques\n",
        "\n",
        "Additional augmentation techniques for better diversity and robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_landmark_variations(landmarks, num_variations=5):\n",
        "    \"\"\"Create variations of landmark data through small perturbations.\"\"\"\n",
        "    landmarks = np.array(landmarks)\n",
        "    variations = []\n",
        "    \n",
        "    for _ in range(num_variations):\n",
        "        # Add small Gaussian noise\n",
        "        noise = np.random.normal(0, 0.01, landmarks.shape)\n",
        "        noisy_landmarks = landmarks + noise\n",
        "        \n",
        "        # Slight scaling\n",
        "        scale_factor = np.random.uniform(0.98, 1.02)\n",
        "        scaled_landmarks = noisy_landmarks * scale_factor\n",
        "        \n",
        "        variations.append(scaled_landmarks)\n",
        "    \n",
        "    return variations\n",
        "\n",
        "def apply_advanced_augmentation(output_dir):\n",
        "    \"\"\"Apply advanced augmentation techniques.\"\"\"\n",
        "    print(\"\\nApplying advanced augmentation techniques...\")\n",
        "    \n",
        "    # 1. Landmark variations for very small classes\n",
        "    for data_type in ['LSTM', 'FCNN']:\n",
        "        data_dir = os.path.join(output_dir, f'{data_type}_Data')\n",
        "        \n",
        "        X_file = os.path.join(data_dir, 'X_train_smote.npy')\n",
        "        y_file = os.path.join(data_dir, 'y_train_smote.npy')\n",
        "        \n",
        "        if os.path.exists(X_file) and os.path.exists(y_file):\n",
        "            X = np.load(X_file)\n",
        "            y = np.load(y_file)\n",
        "            \n",
        "            current_dist = Counter(y)\n",
        "            very_small_classes = [cls for cls, count in current_dist.items() \n",
        "                                if count < TARGET_SAMPLES_PER_CLASS // 2]\n",
        "            \n",
        "            if very_small_classes:\n",
        "                print(f\"  Creating landmark variations for {data_type}: {very_small_classes}\")\n",
        "                \n",
        "                X_augmented = list(X)\n",
        "                y_augmented = list(y)\n",
        "                \n",
        "                for emotion in very_small_classes:\n",
        "                    emotion_indices = np.where(y == emotion)[0]\n",
        "                    \n",
        "                    for idx in emotion_indices:\n",
        "                        variations = create_landmark_variations(X[idx], num_variations=3)\n",
        "                        for variation in variations:\n",
        "                            X_augmented.append(variation)\n",
        "                            y_augmented.append(emotion)\n",
        "                \n",
        "                # Save enhanced data\n",
        "                X_final = np.array(X_augmented)\n",
        "                y_final = np.array(y_augmented)\n",
        "                \n",
        "                np.save(os.path.join(data_dir, 'X_train_enhanced.npy'), X_final)\n",
        "                np.save(os.path.join(data_dir, 'y_train_enhanced.npy'), y_final)\n",
        "                \n",
        "                print(f\"    Enhanced {data_type} shape: {X_final.shape}\")\n",
        "    \n",
        "    # 2. Mixed augmentation for images (combining multiple techniques)\n",
        "    cnn_dir = os.path.join(output_dir, 'CNN_Data')\n",
        "    X_file = os.path.join(cnn_dir, 'X_train_augmented.npy')\n",
        "    y_file = os.path.join(cnn_dir, 'y_train_augmented.npy')\n",
        "    \n",
        "    if os.path.exists(X_file) and os.path.exists(y_file):\n",
        "        X = np.load(X_file)\n",
        "        y = np.load(y_file)\n",
        "        \n",
        "        print(f\"  Applying mixed augmentation to CNN data...\")\n",
        "        \n",
        "        # Create more diverse augmentations for still-minority classes\n",
        "        current_dist = Counter(y)\n",
        "        still_minority = [cls for cls, count in current_dist.items() \n",
        "                         if count < TARGET_SAMPLES_PER_CLASS * 0.8]\n",
        "        \n",
        "        if still_minority:\n",
        "            print(f\"    Adding mixed augmentations for: {still_minority}\")\n",
        "            \n",
        "            X_mixed = list(X)\n",
        "            y_mixed = list(y)\n",
        "            \n",
        "            for emotion in still_minority:\n",
        "                emotion_indices = np.where(y == emotion)[0]\n",
        "                needed = TARGET_SAMPLES_PER_CLASS - current_dist[emotion]\n",
        "                \n",
        "                if needed > 0:\n",
        "                    for _ in range(min(needed, 100)):  # Limit to avoid memory issues\n",
        "                        # Random image from this emotion\n",
        "                        idx = np.random.choice(emotion_indices)\n",
        "                        img = X[idx].copy()\n",
        "                        \n",
        "                        # Apply multiple augmentations\n",
        "                        if np.random.random() > 0.5:\n",
        "                            img = add_noise_augmentation(img, 0.05)\n",
        "                        \n",
        "                        # Color jittering\n",
        "                        if np.random.random() > 0.5:\n",
        "                            brightness_factor = np.random.uniform(0.8, 1.2)\n",
        "                            img = np.clip(img * brightness_factor, 0, 1)\n",
        "                        \n",
        "                        X_mixed.append(img)\n",
        "                        y_mixed.append(emotion)\n",
        "            \n",
        "            # Save mixed augmented data\n",
        "            X_final = np.array(X_mixed)\n",
        "            y_final = np.array(y_mixed)\n",
        "            \n",
        "            np.save(os.path.join(cnn_dir, 'X_train_mixed.npy'), X_final)\n",
        "            np.save(os.path.join(cnn_dir, 'y_train_mixed.npy'), y_final)\n",
        "            \n",
        "            print(f\"    Mixed CNN shape: {X_final.shape}\")\n",
        "\n",
        "# Apply advanced augmentation\n",
        "apply_advanced_augmentation(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Evaluation and Comparison\n",
        "\n",
        "Analyze the results of data augmentation and create comparison visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_augmentation_results(output_dir):\n",
        "    \"\"\"Evaluate and visualize augmentation results.\"\"\"\n",
        "    print(\"\\nEvaluating augmentation results...\")\n",
        "    \n",
        "    results = {\n",
        "        'Original': {},\n",
        "        'Augmented': {},\n",
        "        'Enhanced': {}\n",
        "    }\n",
        "    \n",
        "    # CNN Data Analysis\n",
        "    print(\"\\nCNN Data Results:\")\n",
        "    cnn_files = {\n",
        "        'Original': os.path.join(CNN_DATA_DIR, 'y_train_images.npy'),\n",
        "        'Augmented': os.path.join(output_dir, 'CNN_Data', 'y_train_augmented.npy'),\n",
        "        'Mixed': os.path.join(output_dir, 'CNN_Data', 'y_train_mixed.npy')\n",
        "    }\n",
        "    \n",
        "    for name, file_path in cnn_files.items():\n",
        "        if os.path.exists(file_path):\n",
        "            y = np.load(file_path)\n",
        "            dist = Counter(y)\n",
        "            results[name]['CNN'] = dist\n",
        "            \n",
        "            print(f\"  {name} CNN distribution:\")\n",
        "            total = len(y)\n",
        "            for emotion, count in sorted(dist.items()):\n",
        "                print(f\"    {emotion}: {count} ({count/total*100:.1f}%)\")\n",
        "            \n",
        "            # Calculate balance metrics\n",
        "            counts = list(dist.values())\n",
        "            balance_ratio = max(counts) / min(counts) if min(counts) > 0 else float('inf')\n",
        "            std_dev = np.std(counts)\n",
        "            print(f\"    Balance ratio: {balance_ratio:.2f}:1, Std dev: {std_dev:.1f}\\n\")\n",
        "    \n",
        "    # LSTM/FCNN Data Analysis\n",
        "    for data_type in ['LSTM', 'FCNN']:\n",
        "        print(f\"\\n{data_type} Data Results:\")\n",
        "        \n",
        "        files = {\n",
        "            'Original': os.path.join(DATA_DIR, f'{data_type}_Data', 'y_train.npy'),\n",
        "            'SMOTE': os.path.join(output_dir, f'{data_type}_Data', 'y_train_smote.npy'),\n",
        "            'Enhanced': os.path.join(output_dir, f'{data_type}_Data', 'y_train_enhanced.npy')\n",
        "        }\n",
        "        \n",
        "        for name, file_path in files.items():\n",
        "            if os.path.exists(file_path):\n",
        "                y = np.load(file_path)\n",
        "                dist = Counter(y)\n",
        "                \n",
        "                if data_type not in results[name]:\n",
        "                    results[name][data_type] = {}\n",
        "                results[name][data_type] = dist\n",
        "                \n",
        "                print(f\"  {name} {data_type} distribution:\")\n",
        "                total = len(y)\n",
        "                for emotion, count in sorted(dist.items()):\n",
        "                    print(f\"    {emotion}: {count} ({count/total*100:.1f}%)\")\n",
        "                \n",
        "                # Calculate balance metrics\n",
        "                counts = list(dist.values())\n",
        "                balance_ratio = max(counts) / min(counts) if min(counts) > 0 else float('inf')\n",
        "                std_dev = np.std(counts)\n",
        "                print(f\"    Balance ratio: {balance_ratio:.2f}:1, Std dev: {std_dev:.1f}\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def create_comparison_visualizations(results, output_dir):\n",
        "    \"\"\"Create comparison visualizations.\"\"\"\n",
        "    print(\"Creating comparison visualizations...\")\n",
        "    \n",
        "    # Create comprehensive comparison plot\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Data Augmentation Results Comparison', fontsize=16)\n",
        "    \n",
        "    plot_configs = [\n",
        "        ('CNN', 'Original', 0, 0),\n",
        "        ('CNN', 'Augmented', 0, 1),\n",
        "        ('CNN', 'Mixed', 0, 2),\n",
        "        ('LSTM', 'Original', 1, 0),\n",
        "        ('LSTM', 'SMOTE', 1, 1),\n",
        "        ('LSTM', 'Enhanced', 1, 2)\n",
        "    ]\n",
        "    \n",
        "    for data_type, version, row, col in plot_configs:\n",
        "        if version in results and data_type in results[version]:\n",
        "            dist = results[version][data_type]\n",
        "            emotions = list(dist.keys())\n",
        "            counts = list(dist.values())\n",
        "            \n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(emotions)))\n",
        "            bars = axes[row, col].bar(emotions, counts, color=colors)\n",
        "            axes[row, col].set_title(f'{data_type} - {version}')\n",
        "            axes[row, col].set_ylabel('Number of Samples')\n",
        "            axes[row, col].tick_params(axis='x', rotation=45)\n",
        "            \n",
        "            # Add count labels\n",
        "            for bar, count in zip(bars, counts):\n",
        "                height = bar.get_height()\n",
        "                axes[row, col].text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,\n",
        "                                   f'{count}', ha='center', va='bottom', fontsize=8)\n",
        "        else:\n",
        "            axes[row, col].text(0.5, 0.5, 'No Data', ha='center', va='center', \n",
        "                              transform=axes[row, col].transAxes, fontsize=12)\n",
        "            axes[row, col].set_title(f'{data_type} - {version} (No Data)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'Analysis', 'augmentation_comparison.png'), \n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Create balance improvement chart\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "    \n",
        "    balance_data = []\n",
        "    \n",
        "    for data_type in ['CNN', 'LSTM', 'FCNN']:\n",
        "        for version in ['Original', 'Augmented', 'SMOTE', 'Enhanced', 'Mixed']:\n",
        "            if version in results and data_type in results[version]:\n",
        "                dist = results[version][data_type]\n",
        "                counts = list(dist.values())\n",
        "                if counts:\n",
        "                    balance_ratio = max(counts) / min(counts)\n",
        "                    balance_data.append({\n",
        "                        'Data Type': data_type,\n",
        "                        'Version': version,\n",
        "                        'Balance Ratio': balance_ratio\n",
        "                    })\n",
        "    \n",
        "    if balance_data:\n",
        "        balance_df = pd.DataFrame(balance_data)\n",
        "        \n",
        "        # Pivot for better visualization\n",
        "        pivot_df = balance_df.pivot(index='Data Type', columns='Version', values='Balance Ratio')\n",
        "        \n",
        "        # Plot\n",
        "        pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
        "        ax.set_title('Balance Ratio Improvement (Lower is Better)')\n",
        "        ax.set_ylabel('Balance Ratio (max/min)')\n",
        "        ax.set_xlabel('Data Type')\n",
        "        ax.legend(title='Version', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        ax.tick_params(axis='x', rotation=0)\n",
        "        \n",
        "        # Add horizontal line for \"good balance\" reference\n",
        "        ax.axhline(y=2, color='red', linestyle='--', alpha=0.7, label='Good Balance Threshold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'Analysis', 'balance_improvement.png'), \n",
        "                    dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def generate_augmentation_report(results, output_dir):\n",
        "    \"\"\"Generate comprehensive augmentation report.\"\"\"\n",
        "    report_path = os.path.join(output_dir, 'Analysis', 'augmentation_report.txt')\n",
        "    \n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"Data Augmentation and Oversampling Report\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Configuration:\\n\")\n",
        "        f.write(f\"  Target samples per class: {TARGET_SAMPLES_PER_CLASS}\\n\")\n",
        "        f.write(f\"  Minimum samples threshold: {MIN_SAMPLES_THRESHOLD}\\n\")\n",
        "        f.write(f\"  Random state: {RANDOM_STATE}\\n\\n\")\n",
        "        \n",
        "        for data_type in ['CNN', 'LSTM', 'FCNN']:\n",
        "            f.write(f\"{data_type} Data Results:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            \n",
        "            for version in ['Original', 'Augmented', 'SMOTE', 'Enhanced', 'Mixed']:\n",
        "                if version in results and data_type in results[version]:\n",
        "                    dist = results[version][data_type]\n",
        "                    total = sum(dist.values())\n",
        "                    \n",
        "                    f.write(f\"\\n{version} distribution:\\n\")\n",
        "                    for emotion, count in sorted(dist.items()):\n",
        "                        percentage = (count / total) * 100\n",
        "                        f.write(f\"  {emotion}: {count} ({percentage:.1f}%)\\n\")\n",
        "                    \n",
        "                    # Balance metrics\n",
        "                    counts = list(dist.values())\n",
        "                    if counts:\n",
        "                        balance_ratio = max(counts) / min(counts)\n",
        "                        std_dev = np.std(counts)\n",
        "                        f.write(f\"  Balance ratio: {balance_ratio:.2f}:1\\n\")\n",
        "                        f.write(f\"  Standard deviation: {std_dev:.1f}\\n\")\n",
        "            \n",
        "            f.write(\"\\n\")\n",
        "        \n",
        "        # Recommendations\n",
        "        f.write(\"\\nRecommendations:\\n\")\n",
        "        f.write(\"=\" * 15 + \"\\n\")\n",
        "        f.write(\"1. Use 'Mixed' version for CNN data if available\\n\")\n",
        "        f.write(\"2. Use 'Enhanced' version for LSTM/FCNN data if available\\n\")\n",
        "        f.write(\"3. Monitor training for overfitting due to synthetic data\\n\")\n",
        "        f.write(\"4. Consider class weights as alternative to oversampling\\n\")\n",
        "        f.write(\"5. Validate results on original test set\\n\")\n",
        "    \n",
        "    print(f\"\\nAugmentation report saved to: {report_path}\")\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_augmentation_results(OUTPUT_DIR)\n",
        "create_comparison_visualizations(results, OUTPUT_DIR)\n",
        "generate_augmentation_report(results, OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Final Dataset Preparation\n",
        "\n",
        "Prepare the final balanced datasets for training with proper documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_final_datasets(output_dir):\n",
        "    \"\"\"Prepare final balanced datasets with clear naming and documentation.\"\"\"\n",
        "    print(\"\\nPreparing final balanced datasets...\")\n",
        "    \n",
        "    final_dir = os.path.join(output_dir, 'Final_Balanced')\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "    \n",
        "    # CNN Data\n",
        "    cnn_final_dir = os.path.join(final_dir, 'CNN_Data')\n",
        "    os.makedirs(cnn_final_dir, exist_ok=True)\n",
        "    \n",
        "    # Choose best CNN version\n",
        "    cnn_versions = [\n",
        "        ('mixed', 'X_train_mixed.npy', 'y_train_mixed.npy'),\n",
        "        ('augmented', 'X_train_augmented.npy', 'y_train_augmented.npy')\n",
        "    ]\n",
        "    \n",
        "    best_cnn = None\n",
        "    for version_name, x_file, y_file in cnn_versions:\n",
        "        x_path = os.path.join(output_dir, 'CNN_Data', x_file)\n",
        "        y_path = os.path.join(output_dir, 'CNN_Data', y_file)\n",
        "        \n",
        "        if os.path.exists(x_path) and os.path.exists(y_path):\n",
        "            best_cnn = (version_name, x_path, y_path)\n",
        "            break\n",
        "    \n",
        "    if best_cnn:\n",
        "        version_name, x_path, y_path = best_cnn\n",
        "        shutil.copy2(x_path, os.path.join(cnn_final_dir, 'X_train_balanced.npy'))\n",
        "        shutil.copy2(y_path, os.path.join(cnn_final_dir, 'y_train_balanced.npy'))\n",
        "        print(f\"  CNN: Using {version_name} version\")\n",
        "        \n",
        "        # Copy val and test\n",
        "        for split in ['val', 'test']:\n",
        "            x_orig = os.path.join(output_dir, 'CNN_Data', f'X_{split}_augmented.npy')\n",
        "            y_orig = os.path.join(output_dir, 'CNN_Data', f'y_{split}_augmented.npy')\n",
        "            \n",
        "            if os.path.exists(x_orig) and os.path.exists(y_orig):\n",
        "                shutil.copy2(x_orig, os.path.join(cnn_final_dir, f'X_{split}_balanced.npy'))\n",
        "                shutil.copy2(y_orig, os.path.join(cnn_final_dir, f'y_{split}_balanced.npy'))\n",
        "    \n",
        "    # LSTM and FCNN Data\n",
        "    for data_type in ['LSTM', 'FCNN']:\n",
        "        type_final_dir = os.path.join(final_dir, f'{data_type}_Data')\n",
        "        os.makedirs(type_final_dir, exist_ok=True)\n",
        "        \n",
        "        # Choose best version\n",
        "        versions = [\n",
        "            ('enhanced', 'X_train_enhanced.npy', 'y_train_enhanced.npy'),\n",
        "            ('smote', 'X_train_smote.npy', 'y_train_smote.npy')\n",
        "        ]\n",
        "        \n",
        "        best_version = None\n",
        "        for version_name, x_file, y_file in versions:\n",
        "            x_path = os.path.join(output_dir, f'{data_type}_Data', x_file)\n",
        "            y_path = os.path.join(output_dir, f'{data_type}_Data', y_file)\n",
        "            \n",
        "            if os.path.exists(x_path) and os.path.exists(y_path):\n",
        "                best_version = (version_name, x_path, y_path)\n",
        "                break\n",
        "        \n",
        "        if best_version:\n",
        "            version_name, x_path, y_path = best_version\n",
        "            shutil.copy2(x_path, os.path.join(type_final_dir, 'X_train_balanced.npy'))\n",
        "            shutil.copy2(y_path, os.path.join(type_final_dir, 'y_train_balanced.npy'))\n",
        "            print(f\"  {data_type}: Using {version_name} version\")\n",
        "            \n",
        "            # Copy val and test\n",
        "            for split in ['val', 'test']:\n",
        "                x_orig = os.path.join(output_dir, f'{data_type}_Data', f'X_{split}_smote.npy')\n",
        "                y_orig = os.path.join(output_dir, f'{data_type}_Data', f'y_{split}_smote.npy')\n",
        "                \n",
        "                if os.path.exists(x_orig) and os.path.exists(y_orig):\n",
        "                    shutil.copy2(x_orig, os.path.join(type_final_dir, f'X_{split}_balanced.npy'))\n",
        "                    shutil.copy2(y_orig, os.path.join(type_final_dir, f'y_{split}_balanced.npy'))\n",
        "                \n",
        "                # Copy metadata if exists\n",
        "                meta_orig = os.path.join(output_dir, f'{data_type}_Data', f'{split}_metadata_smote.csv')\n",
        "                if os.path.exists(meta_orig):\n",
        "                    shutil.copy2(meta_orig, os.path.join(type_final_dir, f'{split}_metadata_balanced.csv'))\n",
        "    \n",
        "    # Create dataset info file\n",
        "    info_file = os.path.join(final_dir, 'dataset_info.txt')\n",
        "    with open(info_file, 'w') as f:\n",
        "        f.write(\"Balanced Dataset Information\\n\")\n",
        "        f.write(\"=\" * 30 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(\"File Structure:\\n\")\n",
        "        f.write(\"  CNN_Data/\\n\")\n",
        "        f.write(\"    ├── X_train_balanced.npy  # Augmented image data\\n\")\n",
        "        f.write(\"    ├── y_train_balanced.npy  # Corresponding labels\\n\")\n",
        "        f.write(\"    ├── X_val_balanced.npy    # Validation images\\n\")\n",
        "        f.write(\"    ├── y_val_balanced.npy    # Validation labels\\n\")\n",
        "        f.write(\"    ├── X_test_balanced.npy   # Test images\\n\")\n",
        "        f.write(\"    └── y_test_balanced.npy   # Test labels\\n\\n\")\n",
        "        \n",
        "        f.write(\"  LSTM_Data/\\n\")\n",
        "        f.write(\"    ├── X_train_balanced.npy  # SMOTE'd landmark sequences\\n\")\n",
        "        f.write(\"    ├── y_train_balanced.npy  # Corresponding labels\\n\")\n",
        "        f.write(\"    ├── X_val_balanced.npy    # Validation landmarks\\n\")\n",
        "        f.write(\"    ├── y_val_balanced.npy    # Validation labels\\n\")\n",
        "        f.write(\"    ├── X_test_balanced.npy   # Test landmarks\\n\")\n",
        "        f.write(\"    └── y_test_balanced.npy   # Test labels\\n\\n\")\n",
        "        \n",
        "        f.write(\"  FCNN_Data/\\n\")\n",
        "        f.write(\"    └── (Same structure as LSTM_Data)\\n\\n\")\n",
        "        \n",
        "        f.write(\"Usage Notes:\\n\")\n",
        "        f.write(\"  1. Use train_balanced files for training\\n\")\n",
        "        f.write(\"  2. Use val_balanced files for validation during training\\n\")\n",
        "        f.write(\"  3. Use test_balanced files for final evaluation\\n\")\n",
        "        f.write(\"  4. Test sets are NOT augmented - they remain original\\n\")\n",
        "        f.write(\"  5. Some training samples are synthetic (generated)\\n\")\n",
        "        f.write(\"  6. Monitor for overfitting due to synthetic data\\n\\n\")\n",
        "        \n",
        "        f.write(f\"Augmentation Parameters:\\n\")\n",
        "        f.write(f\"  Target samples per class: {TARGET_SAMPLES_PER_CLASS}\\n\")\n",
        "        f.write(f\"  Image augmentation: {IMG_AUG_PARAMS}\\n\")\n",
        "        f.write(f\"  SMOTE parameters: {SMOTE_PARAMS}\\n\")\n",
        "    \n",
        "    print(f\"\\nFinal balanced datasets prepared in: {final_dir}\")\n",
        "    print(\"Dataset ready for training!\")\n",
        "    \n",
        "    return final_dir\n",
        "\n",
        "def print_final_summary(final_dir):\n",
        "    \"\"\"Print final summary of balanced datasets.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL BALANCED DATASET SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for data_type in ['CNN', 'LSTM', 'FCNN']:\n",
        "        type_dir = os.path.join(final_dir, f'{data_type}_Data')\n",
        "        \n",
        "        print(f\"\\n{data_type} Data:\")\n",
        "        print(\"-\" * 15)\n",
        "        \n",
        "        for split in ['train', 'val', 'test']:\n",
        "            y_file = os.path.join(type_dir, f'y_{split}_balanced.npy')\n",
        "            \n",
        "            if os.path.exists(y_file):\n",
        "                y = np.load(y_file)\n",
        "                dist = Counter(y)\n",
        "                total = len(y)\n",
        "                \n",
        "                print(f\"  {split.title()} Set ({total} samples):\")\n",
        "                for emotion, count in sorted(dist.items()):\n",
        "                    percentage = (count / total) * 100\n",
        "                    print(f\"    {emotion}: {count} ({percentage:.1f}%)\")\n",
        "                \n",
        "                # Balance metrics\n",
        "                counts = list(dist.values())\n",
        "                if len(counts) > 1:\n",
        "                    balance_ratio = max(counts) / min(counts)\n",
        "                    print(f\"    Balance ratio: {balance_ratio:.2f}:1\")\n",
        "            else:\n",
        "                print(f\"  {split.title()} Set: Not found\")\n",
        "    \n",
        "    print(f\"\\nOutput Directory: {final_dir}\")\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"1. Use these balanced datasets for training your models\")\n",
        "    print(\"2. Monitor training curves for overfitting\")\n",
        "    print(\"3. Validate final performance on original (non-augmented) test sets\")\n",
        "    print(\"4. Consider ensemble methods combining CNN + LSTM/FCNN\")\n",
        "    print(\"5. Experiment with different class weights if needed\")\n",
        "\n",
        "# Prepare final datasets\n",
        "final_dir = prepare_final_datasets(OUTPUT_DIR)\n",
        "print_final_summary(final_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "This notebook has successfully addressed data imbalance using multiple techniques:\n",
        "\n",
        "### Techniques Applied:\n",
        "1. **Image Augmentation**: Rotation, shifting, brightness, zoom, flip, noise for CNN data\n",
        "2. **SMOTE**: Synthetic sample generation for landmark data (LSTM/FCNN)\n",
        "3. **Enhanced Augmentation**: Additional variations for very small classes\n",
        "4. **Mixed Approaches**: Combining multiple augmentation techniques\n",
        "\n",
        "### Key Benefits:\n",
        "- ✅ Balanced class distributions\n",
        "- ✅ Preserved original test sets for unbiased evaluation\n",
        "- ✅ Multiple augmentation strategies for robustness\n",
        "- ✅ Comprehensive evaluation and visualization\n",
        "- ✅ Ready-to-use balanced datasets\n",
        "\n",
        "### Important Considerations:\n",
        "- **Monitor for overfitting** during training due to synthetic data\n",
        "- **Validate on original test sets** for true performance assessment\n",
        "- **Consider class weights** as alternative to oversampling\n",
        "- **Use cross-validation** to ensure robustness\n",
        "\n",
        "The balanced datasets are now ready for training your multimodal emotion recognition models!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}